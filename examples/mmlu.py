#!/usr/bin/env python
# coding: utf-8

# ## Building an MMLU Eval
# 
# This notebook shows how to:
# - Build and run an eval
# - Load the results and into a Pandas Dataframe
# 
# We use the `aigc_evals.elsuite.basic.match:Match` Eval class here to check whether new completions match the correct answer. Under the hood, it will generate a completion with the choice of model for each prompt, check if the completion matches the true answer, then logs a result.


# get_ipython().system('curl -O https://people.eecs.berkeley.edu/~hendrycks/data.tar')
# get_ipython().system('tar -xf data.tar')
# data_path = "data"


# In[ ]:


import pandas as pd
import os
from utils import build_mmlu_data
# 限制并发数目
os.environ['EVALS_THREADS'] = "2"
# 注册路径，不建议更改
registry_path = os.path.join(os.path.dirname(__file__), "../registry")

data_path = r'F:\nlpdata_2023\openai\evals\data'

# 构建数据
build_mmlu_data(data_path,registry_path)

# 评估
os.system('sh_aigc_evals langchain/chat_model/chatglm2-6b-int4 match_mmlu_electrical_engineering --registry_path="E:\\algo_project_2023\\aigc_evals\\registry"')


# # How to process the log events generated by oaieval
# events = "/tmp/evallogs/{log_name}"
#
# with open(events, "r") as f:
#     events_df = pd.read_json(f, lines=True)
#
# matches_df = events_df[events_df.type == "match"].reset_index(drop=True)
# matches_df = matches_df.join(pd.json_normalize(matches_df.data))
# matches_df.correct.value_counts().plot.bar(title="Correctness of generated answers", xlabel="Correctness", ylabel="Count")
#
#
# # In[ ]:
#
#
# # Inspect samples
# for i, r in pd.json_normalize(events_df[events_df.type == "sampling"].data).iterrows():
#     print(f"Prompt: {r.prompt}")
#     print(f"Sampled: {r.sampled}")
#     print("-" * 25)

